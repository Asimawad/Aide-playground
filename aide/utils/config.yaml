# path to the task data directory
data_dir: null

# either provide a path to a plaintext file describing the task
desc_file: null
# or provide the task goal (and optionally evaluation information) as arguments
goal: null
eval: null

log_dir: logs
log_level: INFO
workspace_dir: workspaces

# whether to unzip any archives in the data directory
preprocess_data: True
# whether to copy the data to the workspace directory (otherwise it will be symlinked)
# copying is recommended to prevent the agent from accidentally modifying the original data
copy_data: True

exp_name: null # a random experiment name will be generated if not provided

# settings for code execution
exec:
  timeout: 3600
  agent_file_name: runfile.py
  format_tb_ipython: False

# agent hyperparams
agent:
  # how many improvement iterations to run
  steps: 20
  # total time available to agent
  time_limit: 3600 # 1 hour (purely informational)
  # whether to instruct the agent to use CV (set to 1 to disable)
  k_fold_validation: 5
  # whether to instruct the agent to generate a prediction function
  expose_prediction: False
  # whether to provide the agent with a preview of the data
  data_preview: True
  # whether to convert system messages into user messages (can be useful for some local models)
  convert_system_to_user: False
  # whether to obfuscate that we're doing kaggle
  obfuscate: False
  # Style of prompts to use: "default" (optimized for large models), "simple" (better for smaller OS models)
  prompt_style: "simple" # CHANGED default to simple

  # LLM settings for coding
  code:
    # Examples:
    # model: "vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5" # If using vLLM backend
    # model: "deepseek-ai/deepseek-coder-7b-instruct-v1.5" # If using local HF backend
    model: "vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5" # CHANGED: Example using vLLM
    temp: 0.2

  # LLM settings for evaluating program output / tracebacks
  feedback:
    # Use a capable (possibly closed-source) model for feedback, or a local one if preferred.
    # model: "gpt-4-turbo"
    model: "o3-mini" # Keeping o3-mini as a capable small model example
    # model: "vllm/deepseek-ai/deepseek-coder-7b-instruct-v1.5" # Can also use the same model
    temp: 0.5

  # hyperparameters for the tree search
  search:
    max_debug_depth: 5
    debug_prob: 0.7
    num_drafts: 7

# Configuration for vLLM backend
vllm:
  base_url: "http://localhost:8000/v1" # Default vLLM OpenAI-compatible endpoint
  api_key: "EMPTY" # Usually not needed or dummy

# Configuration for local Hugging Face backend
hf_local:
  use_chat_template: true # Apply model's chat template if available
  load_in_4bit: true      # Attempt 4-bit quantization if CUDA is available
  # top_p: 0.9
  # top_k: 50
  # repetition_penalty: 1.1

